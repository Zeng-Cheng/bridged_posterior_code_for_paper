{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "460fe399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hamiltorch\n",
    "from tqdm.notebook import trange\n",
    "from sklearn import svm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from src.svm_slice_sampling import svm_log_prob, svm_optim_step, svm_sample_missing\n",
    "from src.utils_slice import acc_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa293ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.loadtxt('data/heart_failure_clinical_x_test.txt')\n",
    "X_known = np.loadtxt('data/heart_failure_clinical_x_known.txt')\n",
    "y_test = np.loadtxt('data/heart_failure_clinical_y_test.txt')\n",
    "y_known = np.loadtxt('data/heart_failure_clinical_y_known.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "86b25f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Accuracy: 0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "# accuracy without using missing data\n",
    "clf = svm.LinearSVC(loss='hinge', penalty='l2', dual='auto', max_iter=int(1E9))   \n",
    "clf.fit(X_known, y_known)\n",
    "# predict labels for testing data\n",
    "predicted_labels = clf.predict(X_test)\n",
    "print(\"LinearSVC Accuracy:\", accuracy_score(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3d137529-be08-4d2a-9e4e-a278be7769a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6733333333333333 0.5776\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression using only labeled data\n",
    "p = X_known.shape[1]\n",
    "clf = LogisticRegression(random_state=0).fit(X_known, y_known)\n",
    "y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "# predict labels for testing data\n",
    "print('Logistic regression accuracy:', accuracy_score(y_test, 2 * (y_pred > 0.5) - 1))\n",
    "print('Logistic regression AUC', roc_auc_score(y_test, y_pred))\n",
    "np.savetxt('res_svm/pred_prob_logis.txt', np.array([y_test, y_pred]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce797ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bridged posterior -- Bayesian maximum margin classifier\n",
    "\n",
    "# initial values\n",
    "lam = torch.tensor([1])\n",
    "alpha = torch.tensor([1])\n",
    "\n",
    "num_samples = 1500\n",
    "burn_in = 400\n",
    "batch_size = 35\n",
    "\n",
    "num_missing = X_test.shape[0]\n",
    "beta_trace, b_trace, lam_trace, y_trace = [], [], [], []\n",
    "\n",
    "# initialize the missing values of y using Bernoulli(0.5)\n",
    "y_init = 2 * np.random.binomial(1, 0.5, num_missing) - 1\n",
    "\n",
    "y_all = torch.cat([torch.tensor(y_init), torch.tensor(y_known)]).float() # the first m values are unlabeled\n",
    "X_all = torch.cat([torch.tensor(X_test), torch.tensor(X_known)]).float()\n",
    "beta, b = svm_optim_step(X_all, y_all, lam)\n",
    "logprob = svm_log_prob(beta, b, X_all, y_all, lam)\n",
    "lam_lam = 1\n",
    "accept_lam = np.zeros(num_samples)\n",
    "for t in trange(num_samples):\n",
    "    \n",
    "    y_all, beta, b, logprob = svm_sample_missing(\n",
    "        lam, X_all, y_all, batch_size, num_missing, alpha_mcmc = 0.9\n",
    "    )\n",
    "    lam_prop = lam + torch.randn(1) * lam_lam\n",
    "    if (lam_prop > 0): \n",
    "        beta_prop, b_prop = svm_optim_step(X_all, y_all, lam_prop)\n",
    "        logprob_prop = svm_log_prob(beta_prop, b_prop, X_all, y_all, lam_prop)\n",
    "        if (torch.log(torch.rand(1)) < (logprob_prop - logprob)):\n",
    "            lam = lam_prop\n",
    "            beta = beta_prop\n",
    "            b = b_prop\n",
    "            logprob = logprob_prop\n",
    "            accept_lam[t] = 1\n",
    "\n",
    "    if t % 400 == 0:\n",
    "        print('step: {:d}, accept_rate of lam: {:.3f}, lam lam: {:.3f}'.format(\n",
    "            t, acc_rate(accept_lam, t + 1), lam_lam))\n",
    "\n",
    "    beta_trace.append(beta)\n",
    "    b_trace.append(b)\n",
    "    lam_trace.append(lam)\n",
    "    y_trace.append(y_all.clone().detach())\n",
    "\n",
    "beta_trace, b_trace, lam_trace, y_trace = [\n",
    "    np.stack(list) for list in [beta_trace, b_trace, lam_trace, y_trace]]\n",
    "\n",
    "y_test_pred = (y_trace[:, :X_test.shape[0]] + 1) / 2\n",
    "test_pred_prob = np.mean(y_test_pred[burn_in:], axis = 0)\n",
    "\n",
    "print(\"Bridged AUC:\", roc_auc_score(y_test, test_pred_prob))\n",
    "print(\"Bridged accuracy\", accuracy_score(y_test, 2 * (test_pred_prob > 0.5) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5e4b9ced-9380-4fe8-9958-318b3c8d6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('res_svm/pred_prob_bridged.txt', np.array([y_test, test_pred_prob]).T)\n",
    "np.savetxt('res_svm/beta_trace_heart.txt', beta_trace.reshape((-1, X_known.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066aa4b3-058f-404d-9964-14b22fa65ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the proportion of y_i = 1, and the distance to the decision boundary\n",
    "dist_to_dec_bound, prob_y1 = [], [] \n",
    "w_pred = np.mean(beta_trace[burn_in:, :], 0)\n",
    "b_pred = np.mean(b_trace[burn_in:])\n",
    "\n",
    "num_missing = X_test.shape[0]\n",
    "\n",
    "dist_to_dec_bound.append(X_test @ w_pred + b_pred)\n",
    "prob_y1.append((y_trace[burn_in:, :num_missing] == 1).sum(0) / (num_samples - burn_in))\n",
    "\n",
    "dist_to_dec_bound, prob_y1 = np.stack(dist_to_dec_bound), np.stack(prob_y1)\n",
    "np.savetxt('res_svm/dist_to_dec_bound_heart.txt', dist_to_dec_bound)\n",
    "np.savetxt('res_svm/prob_y1_heart.txt', prob_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1173f-8af0-4d92-8291-20ca9169149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs posterior using HMC using only labeled data\n",
    "# Setting the hyper parameters\n",
    "alpha_lam, beta_lam = 3, 2\n",
    "p = X_known.shape[1]\n",
    "X_known, y_known = torch.tensor(X_known).float(), torch.tensor(y_known).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "alpha = torch.tensor([3])\n",
    "\n",
    "def log_prob_gibbs(params):\n",
    "    beta_param = params[:p]\n",
    "    b_param = params[p]\n",
    "    log_lam_param = params[p+1]\n",
    "    lam_param = torch.exp(log_lam_param)\n",
    "    log_lik = svm_log_prob(beta_param, b_param, X_known, y_known, lam_param, alpha = alpha)\n",
    "    \n",
    "    y_ones = torch.ones(X_known.shape[0])\n",
    "    y_minus_ones = -torch.ones(X_known.shape[0])\n",
    "    res_ones = -alpha * torch.relu(1 - y_ones * (X_known @ beta_param + b_param))\n",
    "    res_minus_ones = -alpha * torch.relu(1 - y_minus_ones * (X_known @ beta_param + b_param))\n",
    "    res_norm_const = torch.logaddexp(res_ones, res_minus_ones).sum()       \n",
    "    return log_lik - res_norm_const\n",
    "\n",
    "beta0 = torch.randn(p) \n",
    "b0 = torch.randn(1)\n",
    "log_lam = torch.log(torch.tensor([0.07]))\n",
    "\n",
    "params_init = torch.zeros(p+2)\n",
    "params_init[:p], params_init[p], params_init[p+1] = beta0, b0, log_lam\n",
    "params_init.requires_grad = True\n",
    "\n",
    "num_samples = 2000\n",
    "L = 5\n",
    "burn = 200\n",
    "step_size = 0.01\n",
    "\n",
    "params_hmc_gibbs = hamiltorch.sample(\n",
    "    log_prob_func=log_prob_gibbs, params_init=params_init, num_samples=num_samples,\n",
    "    step_size=step_size, num_steps_per_sample=L, desired_accept_rate=0.9,\n",
    "    sampler=hamiltorch.Sampler.HMC_NUTS, burn=burn)\n",
    "\n",
    "params_gibbs = torch.stack(params_hmc_gibbs)\n",
    "post_beta_gibbs, post_b_gibbs, post_lambda_gibbs = params_gibbs[:,:p], params_gibbs[:,p], params_gibbs[:,p+1]\n",
    "\n",
    "y_test_pred = []\n",
    "for t in range(num_samples - burn):\n",
    "    y_pred = X_test @ post_beta_gibbs[t, ] + post_b_gibbs[t]\n",
    "    p_i = 1 / (1 + torch.exp(-torch.relu(1 + y_pred) + torch.relu(1 - y_pred)))\n",
    "    y_cand = torch.bernoulli(p_i)\n",
    "    # y_pred = np.where(dec_bound > 0, 1, -1)\n",
    "    y_test_pred.append(y_cand)\n",
    "\n",
    "y_test_pred = np.stack(y_test_pred)\n",
    "test_pred_prob = np.mean(y_test_pred, axis = 0)\n",
    "\n",
    "print('Gibbs AUC', roc_auc_score(y_test, test_pred_prob))\n",
    "print('Gibbs accuracy', accuracy_score(y_test, 2 * (test_pred_prob > 0.5) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b799b773-8d08-444e-bc98-250c6aae2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('res_svm/pred_prob_gibbs.txt', np.array([y_test, test_pred_prob]).T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
